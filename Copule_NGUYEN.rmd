---
title: "Estimations des copules"
author: "NGUYEN Tuan Anh"
date: "2023-04-24"
output:
  html_document:
    df_print: paged
---

```{r}
library(readxl)
library(quantmod)
library(KernSmooth)
library(MASS)
library(VineCopula)
library(corrplot)
library(ggplot2)
library(fitdistrplus)
library(copula)
library(kdecopula)
#install.packages("htmltools")
#update.packages()
library(plotly)
library(plot3D)
library(moments)
library(gridExtra)
library(cubature)
```

```{r}
# Télécharger les cours de l'action Apple
getSymbols("AAPL", from = "2012-01-01", to = "2022-12-31")

# Télécharger les données sur les cours de l'action Microsoft
msft <- getSymbols("MSFT", from = "2012-01-01", to = "2022-12-31", auto.assign = FALSE)

# Calculer les rendements quotidiens pour les deux variables
aapl_ret <- diff(log(Cl(AAPL)))
msft_ret <- diff(log(Cl(msft)))
data <- data.frame(Date = index(AAPL), Cl(AAPL), AAPL = aapl_ret, Cl(msft), MSFT = msft_ret)
data <- na.omit(data)
write.csv(data, file = "datasetdraft.csv", row.names = FALSE)
```

```{r}
# Extraire les 2 variables principales
X <- data$AAPL.Close.1
Y <- data$MSFT.Close.1

n <- length(X)
plot(X, Y, xlab = "AAPL", ylab = "MSFT", main = "La concentration moyenne ", 
     col = "blue", cex = 0.5)

#Distribution empirique de marge
Marge_empirique <- function (X) { 
  n <- length(X)
  Marge_empirique <- rank(X)/(n)
  return (Marge_empirique)
}
F_n <-  Marge_empirique(X)
G_n <-  Marge_empirique(Y)
#A_n <- ecdf(X)(X)
#B_n <- ecdf(Y)(Y)
```


```{r}
#Détecter la dépendence
#a. Diagramme de dispersion
par(mfrow = c(1, 2))
plot(X, Y, xlab = "AAPL", ylab = "MSFT", col = "skyblue2", main = "Digramme de dispersion")
plot(F_n, G_n, xlab = "AAPL", ylab = "MSFT", col = "rosybrown2", main = "Rank-rank plot") 
#on voit que le rank-rank plot des 2 variables est comme celle de la coupule Student
#b. Khi-plot
# Khi-plot

BiCopChiPlot(F_n, G_n, mode = "NULL", ylim = c(-0.5, 0.5), main = "Khi-plot", col = "lightgreen", cex = 0.5)

# K-plot

BiCopKPlot(F_n, G_n, PLOT = TRUE, main = "K-plot", col = "slateblue")
```
```{r}
#I. Tests d'indépendance

#Parce que les deux variables ne sont pas gaussiennes. On va faire les tests non paramétriques

#Test de Spearman
test.spearman <- cor.test(X, Y, method = "spearman", exact = FALSE)
test.spearman
#une corrélation positive forte et statistiquement significative entre ces deux variables

#Test de Kendall
test.kendall <- cor.test(X, Y, method = "kendall")
test.kendall
# une forte corrélation positive entre les deux variables analysées.
#Test de Van der Waerden

#La valuer de la statistique
Van <- 1/length(X) * sum(qnorm(F_n, 0, 1) * qnorm(G_n, 0, 1))
Var_nVan <- 1/(length(X) - 1) * (sum(qnorm(c(1: n) / (n + 1), 0, 1) ^ 2) ^ 2)

t_statisque <- Van / sqrt(Var_nVan) * n
p_value <- (1 - pnorm(t_statisque, 0, 1)) * 2
p_value#p_value = 0 très petite, on rejete l'hypothèse H0

#Tous les 3 tests montrent que les 2 variables sont dépendences
```
```{r}
empirique_Copula <- function(X,Y,u,v,step){
  if(length(X) != length(Y)) stop("X and Y must have equal length")
  n <- length(X)
  p <- 0
  rX <- rank(X) / (n)
  rY <- rank(Y) / (n)
  for (i in 1:n){
    p <- p + ((u - step/2 <= rX[i] &  rX[i] <= u + step/2) & (v - step/2 <= rY[i] & rY[i] <= v + step/ 2))
  }
  return(p / (n * step * step))
}

databrut <- cbind(X,Y)

#Convertir les valeurs des données en rangs, puis les ajuster pour qu'ils soient compris dans l'intervalle [0, 1]
data_rank <- apply(databrut, 2, rank) / (n + 1)
kde <- kde2d(data_rank[,1], data_rank[,2], n = 20)

U1 <- seq(0,1,length.out =20)
U2 <- seq(0,1,length.out =20)
grid <- expand.grid(U1 = U1, U2 = U2)


plot_ly(x = kde$x, y = kde$y, z = kde$z, type = "surface")

z_brut <- matrix(empirique_Copula(data_rank[,1], data_rank[,2], grid$U1, grid$U2, 1/20), nrow = 20, ncol = 20)

z_brut

plot_ly(x = U1, y = U2, z = z_brut, type  = "surface")
```

```{r}
#II.Estimation de copule
#1.Methode parametrique
#Histogramme

plotdist(X, breaks = 20, histo = TRUE, demp = TRUE)
descdist(X, discrete=FALSE, boot=500)


plotdist(Y, histo = TRUE, demp = TRUE)
descdist(Y, discrete=FALSE, boot=500)
#=>X and Y ne semblent suivre aucune distribution populaire

```

```{r}
#on ne peut pas utiliser les valeurs de X parce que:
fit1 <- fitdist(Y, "norm", lower = c(-Inf, 0), start = list(mean = mean(X), sd = sd(X)))

# Test goodness of fit
ks.test(Y, "pnorm", mean = fit1$estimate["mean"], sd = fit1$estimate["sd"])
#ne suivre pas la distribution Normal 
```



```{r}
#2.Semi-Parametrique
#Choisir la copule appropriée pour l’ensemble de données 
selectedCopula <- BiCopSelect(data_rank[,1], data_rank[,2], familyset = NA)
selectedCopula
#Student with parameter 0.59
```

```{r}

model_Copula <- tCopula(param = 0.59, dim = 2, dispstr="un", df=6) 

fitstudent <- fitCopula(model_Copula, data_rank, method = "ml")

fitstudent
rho = coef(fitstudent)[1] #the estimated correlation parameter
#rho = 0.5917759 
df = coef(fitstudent)[2] #the degrees of freedom
#df = 2.922124

#plot_3d <- persp(tCopula(dim=2,rho,df=df),dCopula, col ="salmon")

```

```{r}
#3D visualisation sous la copule Student
# Convertir les valeurs des données en rangs, puis les ajuster pour qu'ils soient compris dans l'intervalle [0, 1]


U1 <- seq(0,1,length.out =20)
U2 <- seq(0,1,length.out =20)
grid <- expand.grid(U1 = U1, U2 = U2)

z_student <- c()
for (i in (1:nrow(grid))){
  z_student <- append(z_student, dCopula(c(grid[i,1], grid[i,2]), model_Copula))
}

z_student <- matrix(z_student, ncol = length(U1), nrow = length(U1))


plot_ly(x = U1, y = U2, z = z_student, type = "surface")

#Test goodness-of-fit
gof.t <- gofCopula(copula = ellipCopula(family = c("t"), df =6, df.fixed = T),x = data_rank)
gof.t

```
 Parce que l'on peut ces deux tail dependencie ou au moins lower tail dependency, nous allons essayer la copule Clayton pour modéliser la dépendance entre ces 2 variables.

```{r}
#Comme les graphs montrent qu'il y a des tail dependency entre les deux variables, on va choisir les familes des copules
# qui peuvent representer ces deux tail dependencie ou au moins lower tail dependency


#1.1 Clayton Copula:

Clayton.fit <- fitCopula(claytonCopula(), data = data_rank, method = 'mpl')
Clayton.fit

Clayton.obs <- claytonCopula(param = 1.335)
Clayton.obs

#result
#alpha 
#1.335 
#The maximized loglikelihood is 540.8 
#Optimization converged

#3D show


z_clayton <- c()
for (i in (1:nrow(grid))){
  z_clayton <- append(z_clayton, dCopula(c(grid[i,1], grid[i,2]), Clayton.obs))
}

z_clayton <- matrix(z_clayton, ncol = length(U1), nrow = length(U1))

plot_ly(x = U1, y = U2, z = z_clayton, type = "surface")



#Test goodness-of-fit

gof.Clayton <- gofCopula( Clayton.obs, data_rank)
gof.Clayton
```
```{r}
#3.Non parametric:

#3.1 Kernel Estimation

#3.1.1 Gaussian Kernel without reflection method

# Define Gaussian kernel function
gaussian_kernel <- function(u, v, h) {
  (1 / (2 * pi * h^2)) * exp(-(u^2 + v^2) / (2 * h^2))
}

kernel_density_estimation <- function(u, v, data, h) {
  n <- nrow(data)
  density <- 0
  for (i in 1:n) {
    density <- density + gaussian_kernel((u - data[i, 1]), (v - data[i, 2]), h)
  }
  density <- density / (n)
  return(density)
}


loglik_func_kernel <- function(data,h){
  n <- nrow(data)
  loglik = sum(log(kernel_density_estimation(data[,1], data[,2],data,h)))
  return(loglik)
}

test_h <- seq(0,0.3, length.out = 10 )
log_lik_ker <- c()
for (i in test_h){
  log_lik_ker <- append(log_lik_ker, loglik_func_kernel(data_rank, i))
}
plot(test_h, log_lik_ker)



par(mfrow = c(2, 2))
plot_ly(x = U1, y = U2, z = matrix(kernel_density_estimation(grid$U1, grid$U2, data_rank, 0.01), nrow  = 20, ncol = 20), type  = "surface")
 plot_ly(x = U1, y = U2, z = matrix(kernel_density_estimation(grid$U1, grid$U2, data_rank, 0.05), nrow  = 20, ncol = 20), type  = "surface")
 plot_ly(x = U1, y = U2, z = matrix(kernel_density_estimation(grid$U1, grid$U2, data_rank, 0.07), nrow  = 20, ncol = 20), type  = "surface")
 plot_ly(x = U1, y = U2, z = matrix(kernel_density_estimation(grid$U1, grid$U2, data_rank, 0.1), nrow  = 20, ncol = 20), type  = "surface")


```

```{r}

#3.1.2.Gaussian Kernel reflection method 
reflection<- function(data){
  augmented_data <- data.frame()
  for (i in c( 1, 0, -1 )){
    if (i == 1){
      col1 <- 2- data[,1]
    }
    else if (i == 0){
      col1 <- data[,1]
    }
    else {
      col1 <- -data[,1]
    }
    for (j in c(1, 0, -1)){
      if (j == 1){
      col2 <- 2- data[,2]
    }
      else if (j == 0){
        col2 <- data[,2]
      }
      else {
        col2 <- -data[,2]
      }
      newdat <- cbind(col1, col2)
      augmented_data <- rbind(augmented_data, newdat)
  }
  }
  return(augmented_data)
}

augmented_data <- reflection(data_rank)
par(mfrow = c(1, 2))
plot(augmented_data, main = "Data reflected", xlab = 'U', ylab = 'V')
plot(data_rank, main = "Data" , xlab = 'U', ylab = 'V')


kernel_density_reflection_estimation <- function(u, v,data, h) {
  n <- nrow(augmented_data)
  density <- 0
  for (i in 1:n) {
    density <- density + gaussian_kernel((u - augmented_data[i, 1]), (v - augmented_data[i, 2]), h)
  }
  density <- density * 9 / (n)
  return(density)
}

test_h <- seq(0,0.3, length.out = 10 )
log_lik_ker <- c()
for (i in test_h){
  log_lik_ker <- append(log_lik_ker, sum(log(kernel_density_reflection_estimation(data_rank[,1], data_rank[,2],augmented_data,i))))
}

plot(test_h, log_lik_ker)

plot_ly(x = U1, y = U2, z = matrix(kernel_density_reflection_estimation(grid$U1, grid$U2, combined_data, 0.01), nrow  = 20, ncol  =20)
, type  = "surface")
plot_ly(x = U1, y = U2, z = matrix(kernel_density_reflection_estimation(grid$U1, grid$U2, combined_data, 0.05), nrow  = 20, ncol  =20)
, type  = "surface")
plot_ly(x = U1, y = U2, z = matrix(kernel_density_reflection_estimation(grid$U1, grid$U2, combined_data, 0.07), nrow  = 20, ncol  =20)
, type  = "surface")
plot_ly(x = U1, y = U2, z = matrix(kernel_density_reflection_estimation(grid$U1, grid$U2, combined_data, 0.1), nrow  = 20, ncol  =20)
, type  = "surface")

loglik_reflected <- sum(log(kernel_density_reflection_estimation(data_rank[,1], data_rank[,2],augmented_data,0.1)))
loglik_reflected
```




```{r}
#3.2 Beta Kernel Estimation

# Bivariate beta kernel function
bivariate_beta_kernel <- function(x,alpha, beta) {
  return((x^(alpha - 1) * (1 - x)^(beta - 1) / beta(alpha, beta)))}


# Bivariate beta kernel density estimation function
bivariate_beta_kernel_density <- function(data, u,v, bandwidth) {
  
  n <- nrow(data)
  alpha_u <- u/bandwidth + 1
  beta_u <- (1-u)/bandwidth + 1
  alpha_v <- v/bandwidth + 1
  beta_v <- (1-v)/bandwidth + 1
  kde <- 0
  for (i in 1:n){
    kde <- kde + bivariate_beta_kernel(data[i,1],alpha_u, beta_u) * bivariate_beta_kernel(data[i,2],alpha_v, beta_v)
  }
  
  return(kde/n)
}

test_h <- seq(0,0.3, length.out = 10 )
log_lik_ker <- c()
for (i in test_h){
  log_lik_ker <- append(log_lik_ker, sum(log(bivariate_beta_kernel_density(data_rank, data_rank[,1], data_rank[,2], i))))
}

plot(test_h, log_lik_ker)


plot_ly(x = U1, y = U2 ,z = matrix(bivariate_beta_kernel_density(data_rank, grid$U1, grid$U2, 0.01), nrow = length(U1), ncol = length(U1)), type  = "surface")
plot_ly(x = U1, y = U2 ,z = matrix(bivariate_beta_kernel_density(data_rank, grid$U1, grid$U2, 0.03), nrow = length(U1), ncol = length(U1)), type  = "surface")
plot_ly(x = U1, y = U2 ,z = matrix(bivariate_beta_kernel_density(data_rank, grid$U1, grid$U2, 0.05), nrow = length(U1), ncol = length(U1)), type  = "surface")
plot_ly(x = U1, y = U2 ,z = matrix(bivariate_beta_kernel_density(data_rank, grid$U1, grid$U2, 0.1), nrow = length(U1), ncol = length(U1)), type  = "surface")

#plot_ly(x = U1, y = U2 ,z = z_beta, type  = "contour")


```




```{r}
#3. Model Comparison
h_gaus <- 0.07
h_reflec <- 0.05
h_beta <- 0.02
squared_error <- function(x, hypothetical_copula) {
  u <- x[1]
  v <- x[2]
  empirical_density <- empirique_Copula(F_n, G_n, u, v, 0.05)
  if (hypothetical_copula == 'gker'){
    hypothetical_density <- kernel_density_estimation(u, v,data_rank, h_gaus)
  }
  
  else if (hypothetical_copula == 'gkerf'){
    hypothetical_density <- kernel_density_reflection_estimation(u, v,data_rank, h_reflec)
  }
  
  else if (hypothetical_copula == 'beta'){
    hypothetical_density <- bivariate_beta_kernel_density(data_rank,u, v, h_beta)
  }
    
  else if (hypothetical_copula == 'student'){
    hypothetical_density <- dCopula(c(u,v), copula = fitstudent@copula)
  }
  else {
    hypothetical_density <- dCopula(c(u,v), copula = Clayton.obs)
  }
  
  return((empirical_density - hypothetical_density)^2)
}


mise_result <- adaptIntegrate(squared_error,lower = c(0, 0),
                              upper = c(1, 1),
                              hypothetical_copula = 'gker', maxEval = 100)

MISE_compare <- data.frame()
for (i in c('gker', 'gkerf','beta', 'student', 'clayton')){
  mise <- adaptIntegrate(squared_error,lower = c(0, 0),upper = c(1, 1),hypothetical_copula = i , maxEval=100)
  MISE_compare <- rbind(MISE_compare, c(i, mise$integral))
}
  
plot_ly(x = MISE_compare[,1], y = MISE_compare[,2], type = 'bar')

```
